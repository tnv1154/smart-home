import os
import cv2
import time
import shutil
import numpy as np
import mediapipe as mp  # D√πng MediaPipe Face Detection
import pygame           # üîä TH√äM: ƒë·ªÉ ki·ªÉm tra mixer ƒëang b·∫≠n hay kh√¥ng

# ====== Import c√°c m√¥-ƒëun n·ªôi b·ªô c·ªßa b·∫°n ======
from AI.src.add_vietnamese_text import AddVietnameseText
from AI.src.classifier import Classifier
from AI.src.facenet import delete_classifier_model
from AI.src.face_center_check import check_face_in_ellipse
from AI.src.face_orientation import FaceOrientation
from AI.src.speech import Speech  # d√πng ph√°t √¢m thanh h∆∞·ªõng d·∫´n
# C·ªë g·∫Øng t∆∞∆°ng th√≠ch c·∫£ hai ki·ªÉu ƒë·∫∑t t√™n h√†m align
try:
    from AI.src.align_data_mediapipe import ailgn_data as _align_runner
except Exception:
    try:
        from AI.src.align_data_mediapipe import align_data as _align_runner
    except Exception:
        _align_runner = None
# =============================================

# ====== C·∫•u h√¨nh d·ª± √°n c·ªßa b·∫°n ======
Base_path = "E:/PythonProjectMain/AI"
print(f"Th∆∞ m·ª•c g·ªëc : {Base_path}")

RAW_FOLDER = os.path.join(Base_path, "DataSet", "FaceData", "raw")
PROCESSED_FOLDER = os.path.join(Base_path, "DataSet", "FaceData", "processed")
MODEL_PATH = os.path.join(Base_path, "Models", "20180402-114759.pb")
OUTPUT_CLASSIFIER = os.path.join(Base_path, "Models", "classifier.pkl")

# Th√¥ng s·ªë camera / hi·ªÉn th·ªã / ch·ª•p
CAMERA_INDEX = 0
DISPLAY_SCALE = 0.7     # ch·ªâ √°p d·ª•ng cho khung HI·ªÇN TH·ªä
DETECT_SCALE = 0.5      # downscale ƒë·ªÅu (fx=fy) ƒë·ªÉ detect cho nhanh (0.5 l√† h·ª£p l√Ω)
MARGIN = 0.05           # n·ªõi r·ªông bbox ƒë·ªÉ √¥m tr·ªçn khu√¥n m·∫∑t

# C·∫•u h√¨nh s·ªë ·∫£nh theo h∆∞·ªõng m·∫∑t
IMAGES_PER_ORIENTATION = 10
ORIENTATION_ORDER = ("Front", "Left", "Right")
NUM_IMAGES = IMAGES_PER_ORIENTATION * len(ORIENTATION_ORDER)  # = 30
CAPTURE_INTERVAL = 0.25  # gi√¢y gi·ªØa 2 l·∫ßn ch·ª•p li√™n ti·∫øp
# ====================================


# ====== Detector MediaPipe, b·ªï sung keypoints ƒë·ªÉ d√πng cho FaceOrientation ======
class MediaPipeFaceDetector:
    def __init__(self, min_conf=0.5, long_range=False):
        # model_selection=0 cho c·ª± ly g·∫ßn; 1 cho c·ª± ly xa
        self.detector = mp.solutions.face_detection.FaceDetection(
            model_selection=1 if long_range else 0,
            min_detection_confidence=min_conf
        )

    def detect_faces(self, image_bgr):
        """
        Tr·∫£ v·ªÅ danh s√°ch dict:
          {
            'box': [x, y, w, h],
            'keypoints': {'left_eye':(x,y), 'right_eye':(x,y), 'nose':(x,y)},
            'score': float
          }
        T√≠nh tr√™n ·∫£nh ƒë·∫ßu v√†o image_bgr.
        """
        h, w = image_bgr.shape[:2]
        rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
        res = self.detector.process(rgb)
        out = []

        if not res.detections:
            return out

        for d in res.detections:
            r = d.location_data.relative_bounding_box
            # clip 0..1 r·ªìi ƒë·ªïi ra pixel
            xmin = max(0.0, r.xmin)
            ymin = max(0.0, r.ymin)
            xmax = min(1.0, r.xmin + r.width)
            ymax = min(1.0, r.ymin + r.height)
            x1 = int(xmin * w)
            y1 = int(ymin * h)
            x2 = int(xmax * w)
            y2 = int(ymax * h)
            bw = max(1, x2 - x1)
            bh = max(1, y2 - y1)

            # keypoints: 0=right_eye, 1=left_eye, 2=nose_tip
            kps = d.location_data.relative_keypoints
            kp_dict = {
                'left_eye': (int(kps[1].x * w), int(kps[1].y * h)),
                'right_eye': (int(kps[0].x * w), int(kps[0].y * h)),
                'nose': (int(kps[2].x * w), int(kps[2].y * h)),
            }

            score = float(d.score[0]) if d.score else 0.0
            out.append({
                'box': [x1, y1, bw, bh],
                'keypoints': kp_dict,
                'score': score
            })
        return out

    def close(self):
        try:
            self.detector.close()
        except Exception:
            pass

    def __del__(self):
        self.close()
# ================================================================


def expand_and_clip(x, y, w, h, W, H, margin=MARGIN):
    """
    N·ªõi r·ªông bbox theo c·∫°nh l·ªõn + clip v√†o khung h√¨nh.
    Tr·∫£ v·ªÅ (x, y, w, h) m·ªõi (int).
    """
    cx = x + w / 2.0
    cy = y + h / 2.0
    s = max(w, h) * (1.0 + margin * 2.0)
    x1 = int(round(cx - s / 2.0))
    y1 = int(round(cy - s / 2.0))
    x2 = int(round(cx + s / 2.0))
    y2 = int(round(cy + s / 2.0))
    x1 = max(0, min(W - 1, x1))
    y1 = max(0, min(H - 1, y1))
    x2 = max(0, min(W - 1, x2))
    y2 = max(0, min(H - 1, y2))
    return x1, y1, max(1, x2 - x1), max(1, y2 - y1)


class FaceAdd:
    def __init__(self, id_employee):
        self.face_add = main(id_employee)


def _run_align(person_id: str):
    if _align_runner is None:
        print("[C·∫¢NH B√ÅO] Kh√¥ng t√¨m th·∫•y h√†m align trong AI.src.align_data_mediapipe. B·ªè qua b∆∞·ªõc c·∫Øt khu√¥n m·∫∑t.")
        return
    try:
        _align_runner(person_id)
    except Exception as e:
        print(f"[L·ªñI] G·ªçi align th·∫•t b·∫°i: {e}")


def main(id_employee: str):
    """Th√™m m·ªõi nh√¢n vi√™n: ch·ª•p ·∫£nh ‚Üí align ‚Üí tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng ‚Üí train classifier"""
    person_id = str(id_employee).strip()
    if not person_id:
        print("ID nh√¢n vi√™n tr·ªëng!")
        return

    person_folder = os.path.join(RAW_FOLDER, person_id)
    if not os.path.exists(person_folder):
        os.makedirs(person_folder, exist_ok=True)
    else:
        print(f"Th∆∞ m·ª•c cho id {person_id} ƒë√£ t·ªìn t·∫°i: {person_folder}")
        overwrite = input("Ti·∫øp t·ª•c v√† ghi ƒë√® d·ªØ li·ªáu (y/n): ").strip().lower()
        if overwrite != "y":
            print("H·ªßy thao t√°c")
            return

    print(f"Chu·∫©n b·ªã ch·ª•p {NUM_IMAGES} ·∫£nh cho id {person_id}...")
    print("Y√™u c·∫ßu: 10 ·∫£nh nh√¨n th·∫≥ng, 10 ·∫£nh xoay tr√°i, 10 ·∫£nh xoay ph·∫£i.")
    print("B·∫Øt ƒë·∫ßu ch·ª•p...  (Nh·∫•n q ƒë·ªÉ tho√°t)")

    detector = MediaPipeFaceDetector(min_conf=0.5, long_range=False)
    face_orientation = FaceOrientation()
    speech = Speech()

    # Kh·ªüi t·∫°o camera
    cap = cv2.VideoCapture(CAMERA_INDEX)
    time.sleep(1.5)

    # üîä V·ª´a m·ªü camera xong: y√™u c·∫ßu ƒë·∫∑t khu√¥n m·∫∑t trong khung (ch·ªâ 1 l·∫ßn)
    speech.Trong_khung_start()

    # ƒê·∫øm ri√™ng theo t·ª´ng h∆∞·ªõng m·∫∑t
    front_count = 0
    left_count = 0
    right_count = 0
    count = 0  # t·ªïng s·ªë ·∫£nh ƒë√£ ch·ª•p (1..30)

    last_capture_time = time.time()
    last_speech_time = 0.0
    SPEECH_INTERVAL = 5.0  # gi√¢y gi·ªØa 2 l·∫ßn n√≥i
    last_target_ori = None  # ƒë·ªÉ ph√°t √¢m thanh m·ªói khi ƒê·ªîI H∆Ø·ªöNG

    def get_target_orientation():
        """Tr·∫£ v·ªÅ h∆∞·ªõng m·∫∑t ƒëang c·∫ßn ch·ª•p ti·∫øp theo."""
        nonlocal front_count, left_count, right_count
        if front_count < IMAGES_PER_ORIENTATION:
            return "Front"
        if left_count < IMAGES_PER_ORIENTATION:
            return "Left"
        if right_count < IMAGES_PER_ORIENTATION:
            return "Right"
        return None

    while True:
        target_ori = get_target_orientation()
        if target_ori is None:
            print("ƒê√£ ch·ª•p ƒë·ªß 30 ·∫£nh (10 th·∫≥ng, 10 tr√°i, 10 ph·∫£i).")
            break

        # üîä N·∫øu pha c·∫ßn ch·ª•p thay ƒë·ªïi (Front ‚Üí Left ‚Üí Right), ph√°t √¢m thanh t∆∞∆°ng ·ª©ng
        if target_ori != last_target_ori:
            # Ch·ªâ ph√°t n·∫øu mixer kh√¥ng ƒëang b·∫≠n (tr√°nh ch·ªìng ti·∫øng)
            if not pygame.mixer.get_busy():
                if target_ori == "Front":
                    speech.Nhin_thang_start()
                elif target_ori == "Left":
                    speech.Xoay_trai_start()
                elif target_ori == "Right":
                    speech.Xoay_phai_start()
                last_speech_time = time.time()
            last_target_ori = target_ori

        ret, frame = cap.read()
        if not ret or frame is None:
            print("Kh√¥ng th·ªÉ ƒë·ªçc frame t·ª´ camera")
            continue

        # ·∫¢nh ƒë·ªÉ detect (downscale ƒë·ªÅu, gi·ªØ t·ªâ l·ªá)
        detect_in = frame
        if DETECT_SCALE != 1.0:
            detect_in = cv2.resize(frame, None, fx=DETECT_SCALE, fy=DETECT_SCALE)

        save_frame = frame.copy()  # l∆∞u ·∫£nh g·ªëc v√†o RAW
        faces = detector.detect_faces(detect_in)
        num_face = len(faces)
        now = time.time()

        # ===== Tr∆∞·ªùng h·ª£p kh√¥ng c√≥ ho·∫∑c nhi·ªÅu h∆°n 1 khu√¥n m·∫∑t =====
        if num_face == 0:
            frame = AddVietnameseText.add_vietnamese_text(
                frame, "Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t", (10, 30),
                font_size=50, font_color=(255, 0, 0)
            )
            # nh·∫Øc l·∫°i ƒë·∫∑t m·∫∑t v√†o khung sau m·ªói SPEECH_INTERVAL
            if (now - last_speech_time >= SPEECH_INTERVAL) and (not pygame.mixer.get_busy()):
                speech.Trong_khung_start()
                last_speech_time = now

        elif num_face > 1:
            frame = AddVietnameseText.add_vietnamese_text(
                frame, "C√≥ nhi·ªÅu h∆°n 1 khu√¥n m·∫∑t trong khung h√¨nh", (10, 30),
                font_size=50, font_color=(255, 0, 0)
            )
            if (now - last_speech_time >= SPEECH_INTERVAL) and (not pygame.mixer.get_busy()):
                speech.Trong_khung_start()
                last_speech_time = now

        # ===== Tr∆∞·ªùng h·ª£p c√≥ ƒë√∫ng 1 khu√¥n m·∫∑t =====
        else:
            # bbox tr√™n detect_in ‚Üí scale ng∆∞·ª£c v·ªÅ frame g·ªëc
            dx, dy, dw, dh = faces[0]['box']
            sx = frame.shape[1] / detect_in.shape[1]
            sy = frame.shape[0] / detect_in.shape[0]
            x = int(round(dx * sx))
            y = int(round(dy * sy))
            w = int(round(dw * sx))
            h = int(round(dh * sy))

            # n·ªõi bbox + clip ƒë·ªÉ √¥m tr·ªçn khu√¥n m·∫∑t
            x, y, w, h = expand_and_clip(
                x, y, w, h,
                frame.shape[1], frame.shape[0],
                margin=MARGIN
            )

            # v·∫Ω bbox
            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)

            # Ki·ªÉm tra v·ªã tr√≠ khu√¥n m·∫∑t trong elip (logic trong face_center_check)
            status_text, status_color, inner_rect, outer_rect, cen, axes = check_face_in_ellipse(
                frame, (x, y, w, h)
            )

            # CH·ªà V·∫º ELIP (kh√¥ng v·∫Ω 2 h√¨nh ch·ªØ nh·∫≠t)
            cv2.ellipse(frame, cen, axes, 0, 0, 360, (220, 220, 220), 5)

            # T√≠nh h∆∞·ªõng m·∫∑t (Front / Left / Right / ...)
            face_ori_label = face_orientation.face_orientation_detection(faces)
            cv2.putText(
                frame, face_ori_label, (x, y - 10),
                cv2.FONT_HERSHEY_SIMPLEX, 0.8, status_color, 2
            )

            # ƒêi·ªÅu ki·ªán v·ªã tr√≠ m·∫∑t trong v√πng OK (theo elip)
            in_frame_ok = (status_text == "OK")

            # ƒêi·ªÅu ki·ªán h∆∞·ªõng m·∫∑t ƒë√∫ng v·ªõi pha ƒëang ch·ª•p
            orientation_ok = (
                (target_ori == "Front" and face_ori_label == "Front") or
                (target_ori == "Left" and face_ori_label == "Left") or
                (target_ori == "Right" and face_ori_label == "Right")
            )

            # ==== LOGIC CH·ª§P ·∫¢NH ====
            if in_frame_ok and orientation_ok and now - last_capture_time >= CAPTURE_INTERVAL:
                # Ch·ªâ ch·ª•p khi ·ªü ƒë√∫ng v√πng OK v√† ƒë√∫ng h∆∞·ªõng
                if target_ori == "Front" and front_count < IMAGES_PER_ORIENTATION:
                    front_count += 1
                elif target_ori == "Left" and left_count < IMAGES_PER_ORIENTATION:
                    left_count += 1
                elif target_ori == "Right" and right_count < IMAGES_PER_ORIENTATION:
                    right_count += 1

                # D√πng bi·∫øn count l√†m STT 1..30 ƒë·ªÉ ƒë·∫∑t t√™n file id_0xx.png
                count += 1
                image_path = os.path.join(
                    person_folder,
                    f"{person_id}_{count:03}.png"  # v√≠ d·ª•: 2_001.png, 2_015.png
                )
                cv2.imwrite(image_path, save_frame)
                last_capture_time = now
                print(
                    f"ƒê√£ ch·ª•p {count}/{NUM_IMAGES} ·∫£nh "
                    f"(Front: {front_count}/{IMAGES_PER_ORIENTATION}, "
                    f"Left: {left_count}/{IMAGES_PER_ORIENTATION}, "
                    f"Right: {right_count}/{IMAGES_PER_ORIENTATION})"
                )
            else:
                # N·∫øu ch∆∞a trong v√πng OK / sai h∆∞·ªõng ‚Üí c√≥ th·ªÉ nh·∫Øc, nh∆∞ng ph·∫£i CH·∫∂N SPAM
                if (now - last_speech_time >= SPEECH_INTERVAL) and (not pygame.mixer.get_busy()):
                    if not in_frame_ok:
                        # ch∆∞a ƒë√∫ng v·ªã tr√≠ trong khung elip
                        speech.Trong_khung_start()
                    else:
                        # trong khung nh∆∞ng sai h∆∞·ªõng so v·ªõi pha hi·ªán t·∫°i
                        if target_ori == "Front" and face_ori_label != "Front":
                            speech.Nhin_thang_start()
                        elif target_ori == "Left" and face_ori_label != "Left":
                            speech.Xoay_trai_start()
                        elif target_ori == "Right" and face_ori_label != "Right":
                            speech.Xoay_phai_start()
                    last_speech_time = now

            # Hi·ªÉn th·ªã text y√™u c·∫ßu + ti·∫øn ƒë·ªô
            viet_target = {
                "Front": "Nh√¨n th·∫≥ng",
                "Left": "Xoay tr√°i",
                "Right": "Xoay ph·∫£i"
            }[target_ori]

            frame = AddVietnameseText.add_vietnamese_text(
                frame,
                f"Y√™u c·∫ßu: {viet_target}",
                (10, 30),
                font_size=50,
                font_color=(0, 255, 0)
            )
            frame = AddVietnameseText.add_vietnamese_text(
                frame,
                f"ƒê√£ ch·ª•p - Th·∫≥ng: {front_count}/10  Tr√°i: {left_count}/10  Ph·∫£i: {right_count}/10",
                (10, 90),
                font_size=40,
                font_color=(0, 255, 0)
            )

        # Hi·ªÉn th·ªã (ch·ªâ resize khi DISPLAY, kh√¥ng ·∫£nh h∆∞·ªüng detect)
        disp_w = int(frame.shape[1] * DISPLAY_SCALE)
        disp_h = int(frame.shape[0] * DISPLAY_SCALE)
        disp = cv2.resize(frame, (disp_w, disp_h))
        cv2.imshow("Face add", disp)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()
    detector.close()

    print("Ch·ª•p ·∫£nh ho√†n t·∫•t")

    # Ti·∫øn h√†nh c·∫Øt khu√¥n m·∫∑t (align)
    print("C·∫Øt khu√¥n m·∫∑t (align)...")
    _run_align(person_id)
    print("C·∫Øt khu√¥n m·∫∑t ho√†n t·∫•t")

    # Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng & hu·∫•n luy·ªán b·ªô ph√¢n lo·∫°i
    print("Hu·∫•n luy·ªán b·ªô ph√¢n lo·∫°i...")
    Classifier(PROCESSED_FOLDER, MODEL_PATH, OUTPUT_CLASSIFIER)
    print("Th√™m m·ªõi nh√¢n vi√™n ho√†n t·∫•t")


def face_re_train(id):
    """
    Hu·∫•n luy·ªán l·∫°i b·ªô ph√¢n lo·∫°i khi nh√¢n vi√™n b·ªã x√≥a:
    - X√≥a th∆∞ m·ª•c processed/<id> (n·∫øu c√≥)
    - X√≥a model classifier c≈©
    - Train l·∫°i
    """
    employees_folder = os.path.join(PROCESSED_FOLDER, str(id))
    if os.path.exists(employees_folder):
        shutil.rmtree(employees_folder)
        print(f"ƒê√£ x√≥a th∆∞ m·ª•c: {employees_folder}")
    else:
        print(f"Th∆∞ m·ª•c kh√¥ng t·ªìn t·∫°i: {employees_folder}")

    delete_classifier_model()
    Classifier(PROCESSED_FOLDER, MODEL_PATH, OUTPUT_CLASSIFIER)
    print("ƒê√£ hu·∫•n luy·ªán l·∫°i b·ªô ph√¢n lo·∫°i")


if __name__ == "__main__":
    emp_id = input("Nh·∫≠p id nh√¢n vi√™n: ").strip()
    if emp_id:
        FaceAdd(emp_id)
    else:
        print("ID tr·ªëng, tho√°t.")
